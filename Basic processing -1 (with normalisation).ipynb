{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish Connection to MySQL Docker Container from Jupyter Notebook\n",
    "\n",
    "# Step 1: Install MySQL connector\n",
    "%pip install mysql-connector-python\n",
    "# Step 2: Import libraries\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    " \n",
    "# Step 3: Connect to MySQL Docker Container\n",
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',       # Docker maps to localhost\n",
    "        port=3307,              # Make sure it's exposed in Docker\n",
    "        user='root',            # MySQL root user\n",
    "        password='root',        # Password you set in Docker\n",
    "        database='Nuclear'       # Your database name\n",
    "    )\n",
    " \n",
    "    if connection.is_connected():\n",
    "        db_info = connection.get_server_info()\n",
    "        print(f\"âœ… Connected to MySQL Server version {db_info}\")\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT DATABASE();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(f\"ðŸŽ¯ You're connected to database: {record[0]}\")\n",
    " \n",
    "except Error as e:\n",
    "    print(f\"âŒ Error in connection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc8594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show tables in the database\n",
    "try:\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SHOW TABLES;\")\n",
    "    tables = cursor.fetchall()\n",
    "    print(\"ðŸ“‹ Tables in database:\")\n",
    "    for table in tables:\n",
    "        print(table)\n",
    " \n",
    "except Error as e:\n",
    "    print(f\"âŒ Query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the structure(schema) of the table\n",
    "try:\n",
    "    cursor = connection.cursor()\n",
    " \n",
    "    # Replace 'users' with your table name\n",
    "    #cursor.execute(\"SELECT * FROM gamma_split;\") #for fetching all data\n",
    "    cursor.execute(\"DESCRIBE gamma_split;\") #for fetching the structure(schema) of the table\n",
    "    rows = cursor.fetchall()\n",
    " \n",
    "    print(\"ðŸ“‹ Data from 'gamma':\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    " \n",
    "except Error as e:\n",
    "    print(f\"âŒ Query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eebae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the data from the MySQL database\n",
    "try:\n",
    "    cursor = connection.cursor()\n",
    " \n",
    "    # Replace 'users' with your table name\n",
    "    cursor.execute(\"SELECT * FROM gamma_normalised_smoothed;\") #for fetching all data\n",
    "#    cursor.execute(\"DESCRIBE neutron;\") #for fetching the structure(schema) of the table\n",
    "    rows = cursor.fetchall()\n",
    " \n",
    "    print(\"ðŸ“‹ Data from 'gamma':\")\n",
    "    i=0\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        i=i+1\n",
    "    print(i) #to count the number of rows fetched from the table \n",
    "except Error as e:\n",
    "    print(f\"âŒ Query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa668fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the maximum of each row from the table using pandas and dataframes\n",
    "#-----------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch all data from the table\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM neutron_split;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Get column names\n",
    "col_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows, columns=col_names)\n",
    "\n",
    "# Find maximum of each row\n",
    "row_max = df.max(axis=1)\n",
    "\n",
    "# Display or use the result\n",
    "print(row_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the maximum of each row from the table using SQL GREATEST() function\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Get column names from the table\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"DESCRIBE neutron_split;\")\n",
    "columns = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Build the GREATEST() query dynamically\n",
    "columns_str = ', '.join(columns)\n",
    "query = f\"SELECT GREATEST({columns_str}) AS row_max FROM neutron_split;\"\n",
    "\n",
    "# Execute the query\n",
    "cursor.execute(query)\n",
    "row_max_list = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Display the result\n",
    "print((row_max_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f841841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Fetch all data from the table\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM gamma_split;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Get column names\n",
    "col_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows, columns=col_names)\n",
    "\n",
    "# Find maximum of each row\n",
    "row_max = df.max(axis=1)\n",
    "\n",
    "# Subtract each value from the row max\n",
    "df_processed = row_max.to_numpy()[:, None] - df.values\n",
    "\n",
    "# Create a new DataFrame with the same columns\n",
    "df_processed = pd.DataFrame(df_processed, columns=col_names)\n",
    "print(df_processed)\n",
    "\n",
    "\n",
    "#Normalisation of the processed data\n",
    "#-------------------------------------------------------------\n",
    "# Min-max normalization for each row\n",
    "df_np = df_processed.values.astype(float)\n",
    "row_min = df_np.min(axis=1, keepdims=True)\n",
    "row_max = df_np.max(axis=1, keepdims=True)\n",
    "# Avoid division by zero\n",
    "denom = np.where((row_max - row_min) == 0, 1, row_max - row_min)\n",
    "df_normalised = (df_np - row_min) / denom\n",
    "\n",
    "# Create a new DataFrame with the same columns\n",
    "df_processed_normalised = pd.DataFrame(df_normalised, columns=col_names)\n",
    "print(df_processed_normalised)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Smoothening the normalised data using moving average\n",
    "#-------------------------------------------------------------\n",
    "# Smooth each pulse (row) using moving average with window size 5\n",
    "window_size = 5\n",
    "\n",
    "def smooth_row(row, window=window_size):\n",
    "    return pd.Series(row).rolling(window, min_periods=1, center=True).mean().values\n",
    "\n",
    "df_smoothed = df_processed.apply(smooth_row, axis=1, result_type='expand')\n",
    "df_smoothed.columns = df_processed.columns\n",
    "\n",
    "print(df_smoothed.head())\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# #without normalisation\n",
    "# # 1. Create the table in MySQL \n",
    "# cursor = connection.cursor()\n",
    "# col_defs = []\n",
    "# for col in df_processed.columns:\n",
    "#     col_defs.append(f\"`{col}` INT\")  \n",
    "# col_defs_str = \", \".join(col_defs)\n",
    "# create_table_sql = f\"CREATE TABLE IF NOT EXISTS neutron_processed ({col_defs_str});\"\n",
    "# cursor.execute(\"DROP TABLE IF EXISTS neutron_processed;\")\n",
    "# cursor.execute(create_table_sql)\n",
    "\n",
    "# # 2. Prepare data for insertion\n",
    "# columns = list(df_processed.columns)\n",
    "# insert_query = f\"INSERT INTO neutron_processed ({', '.join(columns)}) VALUES ({', '.join(['%s']*len(columns))});\"\n",
    "# data = df_processed.astype(int).values.tolist()\n",
    "\n",
    "# # 3. Insert in batches\n",
    "# batch_size = 50  # Adjust as needed\n",
    "# try:\n",
    "#     for start in range(0, len(data), batch_size):\n",
    "#         end = start + batch_size\n",
    "#         batch = data[start:end]\n",
    "#         cursor.executemany(insert_query, batch)\n",
    "#         connection.commit()\n",
    "#         print(f\"Inserted rows {start} to {end-1}\")\n",
    "#     print(\"âœ… Data successfully inserted into table 'neutron_processed'.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Failed to insert data into table 'neutron_processed'. Error: {e}\")\n",
    "\n",
    "#without normalisation but smoothened\n",
    "#-------------------------------------------------------------\n",
    "#with Smoothening and normalisation\n",
    "# 1. Create the table in MySQL \n",
    "cursor = connection.cursor()\n",
    "col_defs = []\n",
    "for col in df_smoothed.columns:\n",
    "    col_defs.append(f\"`{col}` FLOAT\")  # Use FLOAT for all columns; adjust if needed\n",
    "col_defs_str = \", \".join(col_defs)\n",
    "create_table_sql = f\"CREATE TABLE IF NOT EXISTS gamma_smoothed ({col_defs_str});\"\n",
    "cursor.execute(\"DROP TABLE IF EXISTS gamma_smoothed;\")\n",
    "cursor.execute(create_table_sql)\n",
    "\n",
    "# 2. Prepare data for insertion\n",
    "columns = list(df_smoothed.columns)\n",
    "insert_query = f\"INSERT INTO gamma_smoothed ({', '.join(columns)}) VALUES ({', '.join(['%s']*len(columns))});\"\n",
    "data = df_smoothed.astype(float).values.tolist()\n",
    "\n",
    "# 3. Insert in batches\n",
    "batch_size = 100  # Adjust as needed\n",
    "try:\n",
    "    for start in range(0, len(data), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = data[start:end]\n",
    "        cursor.executemany(insert_query, batch)\n",
    "        connection.commit()\n",
    "        print(f\"Inserted rows {start} to {end-1}\")\n",
    "    print(\"âœ… Data successfully inserted into table 'gamma_smoothed'.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to insert data into table 'gamma_smoothed'. Error: {e}\")\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# #with normalisation\n",
    "# # 1. Create the table in MySQL \n",
    "# cursor = connection.cursor()\n",
    "# col_defs = []\n",
    "# for col in df_processed_normalised.columns:\n",
    "#     col_defs.append(f\"`{col}` FLOAT\")  # Use FLOAT for all columns; adjust if needed\n",
    "# col_defs_str = \", \".join(col_defs)\n",
    "# create_table_sql = f\"CREATE TABLE IF NOT EXISTS gamma_processed_normalised ({col_defs_str});\"\n",
    "# cursor.execute(\"DROP TABLE IF EXISTS gamma_processed_normalised;\")\n",
    "# cursor.execute(create_table_sql)\n",
    "\n",
    "# # 2. Prepare data for insertion\n",
    "# columns = list(df_processed_normalised.columns)\n",
    "# insert_query = f\"INSERT INTO gamma_processed_normalised ({', '.join(columns)}) VALUES ({', '.join(['%s']*len(columns))});\"\n",
    "# data = df_processed_normalised.astype(float).values.tolist()\n",
    "\n",
    "# # 3. Insert in batches\n",
    "# batch_size = 100  # Adjust as needed\n",
    "# try:\n",
    "#     for start in range(0, len(data), batch_size):\n",
    "#         end = start + batch_size\n",
    "#         batch = data[start:end]\n",
    "#         cursor.executemany(insert_query, batch)\n",
    "#         connection.commit()\n",
    "#         print(f\"Inserted rows {start} to {end-1}\")\n",
    "#     print(\"âœ… Data successfully inserted into table 'gamma_processed_normalised'.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Failed to insert data into table 'gamma_processed_normalised'. Error: {e}\")\n",
    "\n",
    "# #with Smoothening and normalisation\n",
    "# # 1. Create the table in MySQL \n",
    "# cursor = connection.cursor()\n",
    "# col_defs = []\n",
    "# for col in df_normalised_smoothed.columns:\n",
    "#     col_defs.append(f\"`{col}` FLOAT\")  # Use FLOAT for all columns; adjust if needed\n",
    "# col_defs_str = \", \".join(col_defs)\n",
    "# create_table_sql = f\"CREATE TABLE IF NOT EXISTS neutron_normalised_smoothed ({col_defs_str});\"\n",
    "# cursor.execute(\"DROP TABLE IF EXISTS neutron_normalised_smoothed;\")\n",
    "# cursor.execute(create_table_sql)\n",
    "\n",
    "# # 2. Prepare data for insertion\n",
    "# columns = list(df_normalised_smoothed.columns)\n",
    "# insert_query = f\"INSERT INTO neutron_normalised_smoothed ({', '.join(columns)}) VALUES ({', '.join(['%s']*len(columns))});\"\n",
    "# data = df_normalised_smoothed.astype(float).values.tolist()\n",
    "\n",
    "# # 3. Insert in batches\n",
    "# batch_size = 100  # Adjust as needed\n",
    "# try:\n",
    "#     for start in range(0, len(data), batch_size):\n",
    "#         end = start + batch_size\n",
    "#         batch = data[start:end]\n",
    "#         cursor.executemany(insert_query, batch)\n",
    "#         connection.commit()\n",
    "#         print(f\"Inserted rows {start} to {end-1}\")\n",
    "#     print(\"âœ… Data successfully inserted into table 'neutron_normalised_smoothed'.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Failed to insert data into table 'neutron_normalised_smoothed'. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31471ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting the Average plots for neutron and gamma normalised processed data\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract gamma_processed_normalised\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM gamma_normalised_smoothed;\")\n",
    "gamma_rows = cursor.fetchall()\n",
    "gamma_col_names = [desc[0] for desc in cursor.description]\n",
    "df_gamma = pd.DataFrame(gamma_rows, columns=gamma_col_names)\n",
    "print(df_gamma.head())\n",
    "# Extract neutron_processed_normalised\n",
    "cursor.execute(\"SELECT * FROM neutron_normalised_smoothed;\")\n",
    "neutron_rows = cursor.fetchall()\n",
    "neutron_col_names = [desc[0] for desc in cursor.description]\n",
    "df_neutron = pd.DataFrame(neutron_rows, columns=neutron_col_names)\n",
    "print(df_neutron.head())\n",
    "#Calculate mean for each column (index)\n",
    "gamma_mean = df_gamma.mean(axis=0)\n",
    "neutron_mean = df_neutron.mean(axis=0)\n",
    "gamma_mean_norm = (gamma_mean - gamma_mean.min()) / (gamma_mean.max() - gamma_mean.min())\n",
    "neutron_mean_norm = (neutron_mean - neutron_mean.min()) / (neutron_mean.max() - neutron_mean.min())\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(gamma_mean_norm.values[:150], label='Gamma Mean')\n",
    "plt.plot(neutron_mean_norm.values[:150], label='Neutron Mean')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.title('Mean Comparison: Gamma vs Neutron')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the table if it exists\n",
    "\n",
    "cursor.execute(\"SET SESSION wait_timeout = 28800;\")\n",
    "if not connection.is_connected():\n",
    "    connection.reconnect()\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"DROP TABLE IF EXISTS gamma_processed_normalised;\")\n",
    "connection.commit()\n",
    "print(\"Table dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping specific column from the table\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"ALTER TABLE neutron_processed DROP COLUMN t_max;\")\n",
    "connection.commit()\n",
    "print(\"Column 't_max' removed from table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression for classification of neutron and gamma using charge ratios\n",
    "#-----------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Calculate charge ratios for gamma\n",
    "gamma_short_tail = df_gamma.iloc[:, 0:55].sum(axis=1)   # area under index 30-100\n",
    "gamma_long_tail = df_gamma.iloc[:, 55:105].sum(axis=1)     # area under index 0-100\n",
    "gamma_ratio = (gamma_short_tail / gamma_long_tail).values\n",
    "\n",
    "# 2. Calculate charge ratios for neutron\n",
    "neutron_short_tail = df_neutron.iloc[:, 0:55].sum(axis=1)\n",
    "neutron_long_tail = df_neutron.iloc[:, 55:105].sum(axis=1)\n",
    "neutron_ratio = (neutron_short_tail / neutron_long_tail).values\n",
    "print(gamma_ratio)\n",
    "print(neutron_ratio)\n",
    "# 3. Prepare data and labels\n",
    "X = np.concatenate([gamma_ratio, neutron_ratio]).reshape(-1, 1)\n",
    "y = np.concatenate([np.zeros_like(gamma_ratio), np.ones_like(neutron_ratio)])  # 0: gamma, 1: neutron\n",
    "\n",
    "# 4. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 5. Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 6. Evaluation\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['gamma', 'neutron']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#Diagnosing the missclassified samples\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Find indices of misclassified samples\n",
    "misclassified = np.where(y_test != y_pred)[0]\n",
    "\n",
    "# Get the predicted and true labels for misclassified samples\n",
    "misclassified_true = y_test[misclassified]\n",
    "misclassified_pred = y_pred[misclassified]\n",
    "\n",
    "# Get the indices in the original dataset (if you used shuffle, this is needed)\n",
    "# If you used train_test_split with shuffle=True (default), get indices:\n",
    "_, X_test_idx = train_test_split(\n",
    "    np.arange(len(X)), test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "misclassified_orig_idx = X_test_idx[misclassified]\n",
    "\n",
    "# Separate misclassified gamma and neutron indices\n",
    "gamma_wrong_idx = misclassified_orig_idx[(misclassified_true == 0)][:5]\n",
    "neutron_wrong_idx = misclassified_orig_idx[(misclassified_true == 1)][:5]\n",
    "\n",
    "# Calculate the LR threshold (decision boundary)\n",
    "# For 1D feature: threshold = -intercept_/coef_\n",
    "threshold = -clf.intercept_[0] / clf.coef_[0][0]\n",
    "print(threshold)\n",
    "\n",
    "# Plot 5 misclassified gamma pulses\n",
    "plt.figure(figsize=(21, 21))\n",
    "for i, idx in enumerate(gamma_wrong_idx):\n",
    "    ratio = X[idx][0]\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.plot(df_gamma.iloc[idx % len(df_gamma)].values[:150])\n",
    "    plt.plot(gamma_mean.values[:150], label='Gamma Mean')\n",
    "    plt.title(f\"Gamma (misclassified)\\nIndex: {idx}\\nRatio={ratio:.3f}\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot 5 misclassified neutron pulses\n",
    "for i, idx in enumerate(neutron_wrong_idx):\n",
    "    ratio = X[idx][0]\n",
    "    plt.subplot(2, 5, 5+i+1)\n",
    "    plt.plot(df_neutron.iloc[idx % len(df_neutron)].values[:150])\n",
    "    plt.plot(neutron_mean.values[:150], label='Neutron Mean')\n",
    "    plt.title(f\"Neutron (misclassified)\\nIndex: {idx}\\nRatio={ratio:.3f}\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#plotting the FoM (figure of merit) for the charge ratios\n",
    "#-----------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(gamma_ratio, bins=500, alpha=0.6, label='Gamma', color='blue', density=True)\n",
    "plt.hist(neutron_ratio, bins=500, alpha=0.6, label='Neutron', color='orange', density=True)\n",
    "\n",
    "# Calculate means and FWHMs\n",
    "mean_gamma = np.mean(gamma_ratio)\n",
    "mean_neutron = np.mean(neutron_ratio)\n",
    "std_gamma = np.std(gamma_ratio)\n",
    "std_neutron = np.std(neutron_ratio)\n",
    "fwhm_gamma = 2.355 * std_gamma\n",
    "fwhm_neutron = 2.355 * std_neutron\n",
    "\n",
    "# Calculate FoM\n",
    "fom = abs(mean_neutron - mean_gamma) / (fwhm_neutron + fwhm_gamma)\n",
    "\n",
    "# Annotate means and FoM\n",
    "plt.axvline(mean_gamma, color='blue', linestyle='--', label=f'Gamma Mean: {mean_gamma:.3f}')\n",
    "plt.axvline(mean_neutron, color='orange', linestyle='--', label=f'Neutron Mean: {mean_neutron:.3f}')\n",
    "plt.axvline(threshold, color='red', linestyle=':', label=f'LR Threshold: {threshold:.3f}')\n",
    "plt.title(f'Pulse Shape Discrimination\\nFoM = {fom:.3f}')\n",
    "plt.xlabel('Charge Ratio')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5699b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the best index for short and long tail for classification using logistic regression\n",
    "#-----------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "best_acc = 0\n",
    "best_short = None\n",
    "best_long = None\n",
    "\n",
    "# Define reasonable search ranges for short and long tail indices\n",
    "for short_tail in range(30, 80, 5):   # e.g., 10 to 75 in steps of 5\n",
    "    for long_tail in range(short_tail+10, 120, 5):  # always > short_tail\n",
    "        # Calculate ratios for gamma\n",
    "        gamma_short = df_gamma.iloc[:, short_tail:long_tail+1].sum(axis=1)\n",
    "        gamma_long = df_gamma.iloc[:, 0:long_tail+1].sum(axis=1)\n",
    "        gamma_ratio = (gamma_short / gamma_long).values\n",
    "\n",
    "        # Calculate ratios for neutron\n",
    "        neutron_short = df_neutron.iloc[:, short_tail:long_tail+1].sum(axis=1)\n",
    "        neutron_long = df_neutron.iloc[:, 0:long_tail+1].sum(axis=1)\n",
    "        neutron_ratio = (neutron_short / neutron_long).values\n",
    "\n",
    "        # Prepare data and labels\n",
    "        X = np.concatenate([gamma_ratio, neutron_ratio]).reshape(-1, 1)\n",
    "        y = np.concatenate([np.zeros_like(gamma_ratio), np.ones_like(neutron_ratio)])\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=32, stratify=y\n",
    "        )\n",
    "\n",
    "        # Logistic Regression\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Track best\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_short = short_tail\n",
    "            best_long = long_tail\n",
    "            print(f\"New best: short={short_tail}, long={long_tail}, accuracy={acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest indices: short_tail={best_short}, long_tail={best_long}, accuracy={best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for logistic regression using decay slope comparison\n",
    "#-----------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "best_acc = 0\n",
    "best_short = None\n",
    "best_long = None\n",
    "\n",
    "# Define reasonable search ranges for short and long tail indices\n",
    "for short_tail in range(30, 80, 5):   # e.g., 10 to 75 in steps of 5\n",
    "    for long_tail in range(short_tail+10, 120, 5):  # always > short_tail\n",
    "        # Calculate ratios for gamma\n",
    "        gamma_short = df_gamma.iloc[:, short_tail]\n",
    "        gamma_long = df_gamma.iloc[:, long_tail]\n",
    "\n",
    "        gamma_ratio = ((gamma_short-gamma_long)/(long_tail-short_tail)).values\n",
    "\n",
    "        # Calculate ratios for neutron\n",
    "        neutron_short = df_neutron.iloc[:, short_tail]\n",
    "        neutron_long = df_neutron.iloc[:, long_tail]\n",
    "\n",
    "        neutron_ratio = ((neutron_short-neutron_long)/(long_tail-short_tail)).values\n",
    "\n",
    "        # Prepare data and labels\n",
    "        X = np.concatenate([gamma_ratio, neutron_ratio]).reshape(-1, 1)\n",
    "        y = np.concatenate([np.zeros_like(gamma_ratio), np.ones_like(neutron_ratio)])\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=32, stratify=y\n",
    "        )\n",
    "\n",
    "        # Logistic Regression\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"short={short_tail}, long={long_tail}, accuracy={acc:.4f}\")\n",
    "        # Track best\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_short = short_tail\n",
    "            best_long = long_tail\n",
    "            print(f\"New best: short={short_tail}, long={long_tail}, accuracy={acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest indices: short_tail={best_short}, long_tail={best_long}, accuracy={best_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
